# -*- coding: utf-8 -*-
"""MBA933 - Telco-Customer-Churn - Decision Tree v1.00 Backup_Post_Running - Copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ive1edoX_E71Cp4mnsg4GsC0FksyuqZF
"""

# Deepak Saini

print("Start of Code")

# uninstall old version of scikit-learn, and install latest version of scikit-learn
!pip uninstall scikit-learn -y
!pip install -U scikit-learn
import sklearn

# confirm the current scikit-learn version
sklearn.__version__

# import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# import scikit-learn function 'train_ test_split'
from sklearn.model_selection import train_test_split

# import scikit-learn function 'Decision Tree Classifier'
from sklearn.tree import DecisionTreeClassifier

# import scikit-learn function 'plot_tree' (it requires matplotlib to be installed)
from sklearn.tree import plot_tree

# import scikit-learn module 'metrics' (this module implements functions assessing prediction error for specific purposes, used for accuracy calculation)
from sklearn import metrics

# import scikit-learn function 'plot_confusion_matrix' to plot the confusion matrix
# from sklearn.metrics import plot_confusion_matrix
# Note: Function plot_confusion_matrix is deprecated in scikit-learn version 1.0 and removed in scikit-learn version 1.2 (latest version is 1.2.1 on 09-Feb-2023)

# import scikit-learn function 'ConfusionMatrixDisplay' to plot the confusion matrix
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

# import scikit-learn function 'accuracy_score' to calculate the accuracy score
from sklearn.metrics import accuracy_score

# ignore warnings
import warnings
warnings.filterwarnings ('ignore')

# mount google drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# read the excel file data to create a pandas dataframe
path='/content/drive/MyDrive/IITK/MBA933 - Telco-Customer-Churn/Datafile.xlsx'
df = pd.read_excel(path)
df

# view the shape of dataframe
print(df.shape)
num_rows , num_cols = df.shape
print(num_rows)
print(num_cols)
print("There are {} rows and {} columns.".format(df.shape[0],df.shape[1]))

# view name of columns in dataframe
list(df)

# view the datatypes in dataframe
df.dtypes

# view a sample of values in each column of dataframe
pd.Series({col:df[col].unique() for col in df})

# view the count of unique values in each column of dataframe
# [Note: for data field column 'customerID', the count should equal to the number of rows in dataframe, to confirm that there is no duplicate records (tuples) in customer data]
df.describe(include='all').loc['unique', :]

# view unique values in each column of dataframe
for col in df:
    print(col)
    print(df[col].unique())
    print("-------------------------------------------------------------------------")

# view the information about the data frame
df.info()

# convert data field column 'SeniorCitizen' from numeric to categorical variable
df['SeniorCitizen'] = df['SeniorCitizen'].astype(np.object)

# convert integer type numeric variable 'tenure' to float type numeric variable, to enable use of fit() function
df['tenure'] = df['tenure'].astype(np.float64)

# view the information about the data frame, with column 'SeniorCitizen' as categorical variable, and column 'tenure' as float type numeric variable
df.info()

# check for null values in dataframe
df.isnull().sum()

# view the statistical description of dataframe (for numeric columns)
df.describe()

# view the count of values in data field column 'Churn'
df['Churn'].value_counts()

# plot to visualize the the count of values in data field column 'Churn', using the seaborn library
plt.figure(figsize=(5,10));
sns.countplot(df['Churn']);

# separate the independent variables (Xs) from dataframe, and check using 'head' [Note: 'customerID', being Nominal Data, is not included in Xs]
X = df.drop(['customerID', 'Churn'], axis=1).copy()
X.head()

# view the datatypes for Xs
X.dtypes

# One-Hot Encoding to convert categorical variables into dummy/indicator variables [Note: there should be 46 columns after One-Hot Encoding, as per the count of unique values in each column (as identified above)]
X_encoded = pd.get_dummies(X,columns=['gender', 'SeniorCitizen', 'Partner', 'Dependents',
                                      'PhoneService', 'MultipleLines', 'InternetService',
                                      'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                                      'TechSupport',
                                      'StreamingTV', 'StreamingMovies',
                                      'Contract', 'PaperlessBilling', 'PaymentMethod'])
X_encoded.head()

# separate the dependent variable (Y) from dataframe, and check using 'head'
Y = df['Churn'].copy()
Y.head()

# view the datatype for Y
Y.dtype

# dtype('O') stands for (Python) objects, which in Pandas refers to a string (text)
# convert Y from categorical to float type numeric data, to enable use of fit() function

# for Y, map the values 'Yes' and 'No' to numeric values (to enable conversion of categorical variable Y to float type numeric variable), and check using 'head'
Y = Y.map({'No': 0, 'Yes': 1})
Y = Y.astype(np.float64)
Y.head()

# Check datatype for Y
Y.dtype

# Check the unique values for Y
Y.unique()

# split the data into training data and test data
X_train, X_test, Y_train, Y_test = train_test_split(X_encoded, Y, test_size = 0.3, random_state = 0)

print("Completed 'train_test_split'.")

# create a decision tree using class 'DecisionTreeClassifier'
# Decision trees are a popular supervised learning method
decision_tree = DecisionTreeClassifier(random_state = 0)
decision_tree = decision_tree.fit(X_train, Y_train)

print("Fully grown decision tree has been created.")

# plot the decision tree using function 'plot_tree'
# [Note: Parameter 'class_names' contains names of each of the target classes in ascending numerical order.
#        Here, for data field column 'Churn', 'No' <=> 0 <=> "Churning_No", and 'Yes' <=> 1 <=> "Churning_Yes".]
plt.figure(figsize = (20, 15))
plot_tree (decision_tree, filled = True, rounded = True, class_names = ["Churning_No", "Churning_Yes"], feature_names = X_encoded.columns);

print("Fully grown decision tree has been plotted.")

# plot the confusion matrix

#plot_confusion_matrix(decision_tree,X_test, Y_test, display_labels=[["gender", "SeniorCitizen", "Partner", "Dependents", "PhoneService", "MultipleLines", "InternetService", "OnlineSecurity", "OnlineBackup", "DeviceProtection", "TechSupport", "StreamingTV", "StreamingMovies", "Contract", "PaperlessBilling", "PaymentMethod"]])

# predict the labels of the test set
Y_pred = decision_tree.predict(X_test)

# compute the confusion matrix
cm = confusion_matrix(Y_test, Y_pred)

# create a confusion matrix display object
cm_display = ConfusionMatrixDisplay(cm, display_labels=["No", "Yes"])

# plot the confusion matrix
cm_display.plot()

# view the count of a values for Y_train
Y_train.value_counts()

# performing cost complexity pruning on decision tree model
# the 'alpha' values represent a complexity parameter for balancing between (a) the accuracy of model (using the training data), and (b) the complexity of model
path = decision_tree.cost_complexity_pruning_path(X_train, Y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# plot to visualize the relationship between the effective 'alpha' values and the total impurity of the leaves of the decision tree classifier for the training set
fig, ax = plt.subplots()
ax. plot(ccp_alphas[:-1], impurities[:-1], marker = "o" , drawstyle = "steps-post")
ax.set_xlabel("Effective Alpha")
ax.set_ylabel("Total Impurity of Leaves" )
ax.set_title("Total Impurity of Leaves vs. Effective Alpha for the Training Set")

# build decision tree classifiers with varying values of effective 'alpha'
# obtain the number of nodes in the last (i.e. the most complex) decision tree, along with the corresponding value of 'alpha'
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alpha)
    clf.fit(X_train, Y_train)
    clfs.append(clf)

print("The number of nodes in the last tree is {}, with ccp_alpha as {}.".format(
          clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)

# plot to visualize the relationship between the effective 'alpha' values and the number of nodes in the decision tree model
# plot to visualize the relationship between the effective 'alpha' values and the depth of the decision tree model

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker = "o", drawstyle = "steps-post")
ax[0].set_xlabel("Effective Alpha")
ax[0].set_ylabel("Number of Nodes")
ax[0].set_title("Number of Nodes vs. Effective Alpha")
ax[1].plot(ccp_alphas, depth, marker = "o" , drawstyle = "steps-post")
ax[1].set_xlabel("Effective Alpha")
ax[1].set_ylabel("Depth of Tree")
ax[1].set_title("Depth of Tree vs. Effective Alpha")
fig.tight_layout()

# plotting the accuracy scores for the training set and testing set against different values of effective 'alpha'

train_scores = [clf.score(X_train, Y_train) for clf in clfs]
test_scores = [clf.score(X_test, Y_test) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("Effective Alpha")
ax.set_ylabel("Accuracy")
ax.set_title("Accuracy vs. Effective Alpha for Training Set and Testing Set" )
ax.plot(ccp_alphas, train_scores, marker = "o", label = "Training Set", drawstyle = "steps-post")
ax.plot(ccp_alphas, test_scores, marker = "o", label = "Testing Set", drawstyle = "steps-post")
ax.legend()
plt.show()

# finding the value of effective 'alpha' for which accuracy of the decision tree model is maximum on testing set
max_accuracy = max(test_scores)
max_accuracy_index = test_scores.index(max_accuracy)
best_alpha = ccp_alphas[max_accuracy_index]
print("The maximum Accuracy on the Testing Set is {}.".format(max_accuracy))
print("The value of Effective Alpha corresponding to the maximum Accuracy on the Testing Set is {}.".format(best_alpha))

# creating a decision tree using the value of effective 'alpha' for which accuracy of the decision tree model is maximum on Testing Set

# finding the value of effective 'alpha' for which accuracy is maximum
max_accuracy_alpha = ccp_alphas[test_scores.index(max(test_scores))]

# creating a decision tree using the value of effective 'alpha'for which accuracy is maximum
decision_tree_pruned = DecisionTreeClassifier(random_state = 0, ccp_alpha = max_accuracy_alpha)
decision_tree_pruned.fit(X_train, Y_train)

# plot the pruned decision tree using function 'plot_tree'
# [Note: Parameter 'class_names' contains names of each of the target classes in ascending numerical order.
#        Here, for data field column 'Churn', 'No' <=> 0 <=> "Churning_No", and 'Yes' <=> 1 <=> "Churning_Yes".]
plt.figure(figsize = (30, 15))
plot_tree (decision_tree_pruned, filled = True, rounded = True, class_names = ["Churning_No", "Churning_Yes"], feature_names = X_encoded.columns, fontsize=12);

print("Pruned decision tree has been created by using postpruning approach (minimal cost complexity pruning).")

print("Code ran sucessfully!")

print("End of Code")

# To Restart Runtime
# exit()